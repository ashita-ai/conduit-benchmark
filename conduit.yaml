# Conduit Configuration
# See docs/configuration.md for detailed documentation

routing:
  # Default optimization strategy
  # Options: balanced, quality, cost, speed
  default_optimization: balanced

  # Reward weight presets for routing decisions
  # Each preset balances quality, cost, and latency differently
  # Weights must sum to 1.0
  presets:
    balanced:
      quality: 0.7    # Prioritize quality
      cost: 0.2       # Moderate cost concern
      latency: 0.1    # Low latency concern

    quality:
      quality: 0.8    # Maximize quality
      cost: 0.1       # Minimal cost concern
      latency: 0.1    # Minimal latency concern

    cost:
      quality: 0.4    # Acceptable quality
      cost: 0.5       # Minimize cost
      latency: 0.1    # Low latency concern

    speed:
      quality: 0.4    # Acceptable quality
      cost: 0.1       # Low cost concern
      latency: 0.5    # Minimize latency

# Context-specific priors for Thompson Sampling cold start optimization.
# These priors help the bandit algorithm make better routing decisions before
# it has collected enough data to learn model performance empirically.
#
# How it works:
#   - Quality scores (0.0-1.0) represent expected success rate for each context
#   - Scores are converted to Beta(alpha, beta) distribution parameters
#   - Higher score = more confident the model performs well in that context
#   - Prior strength is ~10,000 samples, so real observations gradually override
#
# Example: claude-sonnet-4.5: 0.92 for "code" means we expect 92% quality
# on coding tasks, giving it a head start until real feedback proves otherwise.
#
# Model names must match your configured models in default_models above.
priors:
  code:
    claude-sonnet-4.5: 0.92    # SWE-Bench leader, best for code
    claude-opus-4.5: 0.90      # Premium coding capability
    gpt-5.1: 0.88              # Strong coding performance
    gemini-2.5-pro: 0.85       # Good coding alternative
    gpt-5-mini: 0.80           # Fast, cost-effective coding
    gemini-2.5-flash: 0.75     # Quick coding responses
    claude-haiku-4.5: 0.73     # Fast, budget-friendly
    gpt-5-nano: 0.70           # Minimal but capable

  creative:
    claude-opus-4.5: 0.94      # Excellent creativity
    claude-sonnet-4.5: 0.90    # Strong creative writing
    gpt-5.1: 0.86              # Good creative output
    gemini-2.5-pro: 0.82       # Good creative alternative
    gpt-5-mini: 0.76           # Acceptable creative
    claude-haiku-4.5: 0.72     # Budget creative
    gemini-2.5-flash: 0.70     # Fast creative responses
    gpt-5-nano: 0.65           # Basic creative tasks

  analysis:
    claude-opus-4.5: 0.92      # Deep analytical ability
    gpt-5.1: 0.89              # Strong analysis
    claude-sonnet-4.5: 0.88    # Great for analysis
    gemini-2.5-pro: 0.85       # Strong reasoning
    gpt-5-mini: 0.78           # Good for most analysis
    claude-haiku-4.5: 0.74     # Lighter analysis
    gemini-2.5-flash: 0.72     # Fast analytical responses
    gpt-5-nano: 0.68           # Basic analytical tasks

  simple_qa:
    gpt-5-nano: 0.90           # Optimized for simple QA
    gpt-5-mini: 0.88           # Fast and accurate
    gemini-2.5-flash: 0.85     # Great for simple QA
    claude-haiku-4.5: 0.84     # Fast, accurate, cheap
    gpt-5.1: 0.82              # Reliable but overkill
    claude-sonnet-4.5: 0.80    # Good but unnecessary
    gemini-2.5-pro: 0.78       # Too powerful for simple QA
    claude-opus-4.5: 0.75      # Unnecessary power

  general:
    gpt-5.1: 0.88              # Best all-rounder
    claude-opus-4.5: 0.87      # Premium quality
    claude-sonnet-4.5: 0.85    # Balanced performance
    gemini-2.5-pro: 0.83       # Strong alternative
    gpt-5-mini: 0.80           # Cost-effective
    claude-haiku-4.5: 0.76     # Budget-friendly
    gemini-2.5-flash: 0.74     # Fast and affordable
    gpt-5-nano: 0.70           # Minimal general use

# Algorithm Hyperparameters
# Configure exploration/exploitation trade-offs for bandit algorithms
algorithms:
  linucb:
    alpha: 1.0                    # Exploration parameter (higher = more exploration)
    success_threshold: 0.85       # Reward threshold for "success" in statistics

  epsilon_greedy:
    epsilon: 0.1                  # Exploration rate (10% random exploration)
    decay: 1.0                    # Epsilon decay rate (1.0 = no decay)
    min_epsilon: 0.01             # Minimum epsilon floor

  ucb1:
    c: 1.5                        # Confidence multiplier for upper bound

  thompson_sampling:
    lambda: 1.0                   # Regularization parameter (Bayesian prior strength)

# Embedding Provider Configuration
# Controls how query embeddings are generated for routing decisions
# Priority: conduit.yaml settings > environment variables > auto-detection
embeddings:
  # Provider selection (auto, openai, cohere, fastembed, sentence-transformers, huggingface)
  # - auto: Auto-detect in priority order (OpenAI → Cohere → FastEmbed → sentence-transformers)
  # - openai: Requires OPENAI_API_KEY (recommended - reuses LLM key)
  # - cohere: Requires COHERE_API_KEY
  # - fastembed: Local ONNX models (~100MB, pip install fastembed)
  # - sentence-transformers: Local PyTorch models (~2GB, pip install sentence-transformers)
  # - huggingface: HuggingFace Inference API (requires HF_TOKEN)
  provider: auto

  # Model selection (provider-specific, null = use provider default)
  # OpenAI: text-embedding-3-small (1536 dims), text-embedding-3-large (3072 dims)
  # Cohere: embed-english-v3.0 (1024 dims), embed-multilingual-v3.0 (1024 dims)
  # FastEmbed: BAAI/bge-small-en-v1.5 (384 dims)
  # sentence-transformers: all-MiniLM-L6-v2 (384 dims)
  model: null

  # PCA dimensionality reduction (improves LinUCB convergence)
  # PCA models are auto-generated per provider on first use (~5 seconds)
  # and cached at ~/.cache/conduit/pca_{provider}.pkl
  #
  # Provider-specific recommendations (variance vs convergence trade-off):
  #   FastEmbed/sentence-transformers (384-dim): 64 components → 95% variance, ~600 queries
  #   OpenAI (1536-dim): 128 components → 73% variance, ~1300 queries
  #                      192 components → 85% variance, ~1900 queries
  #   Cohere (1024-dim): 96 components → 80% variance, ~1000 queries
  pca:
    enabled: false                # Enable PCA compression (default: false)
    components: 128               # Number of principal components (provider-dependent, see above)
    auto_retrain: true            # Retrain PCA on your workload during UCB1 phase
    retrain_threshold: 150        # Minimum queries before auto-retraining

# Feature Dimensions
# Controls dimensionality of feature vectors for bandit algorithms
# TODO: Auto-detect from embedding provider at runtime (see GitHub issue #53)
features:
  embedding_dim: 384              # Base embedding dimension (provider-dependent)
  full_dim: 386                   # Full feature dimension (embedding + 2 metadata)
  pca_dim: 66                     # PCA feature dimension (64 components + 2 metadata)
  token_count_normalization: 1000.0  # Divisor for token count normalization

# Quality Estimation Configuration
# Used for heuristic quality scoring in LiteLLM feedback when no explicit rating
quality_estimation:
  base_quality: 0.9               # Quality for successful responses
  empty_quality: 0.1              # Quality for empty responses
  failure_quality: 0.1            # Quality for error responses
  min_response_chars: 10          # Minimum valid response length

  # Quality penalties (deducted from base_quality)
  penalties:
    short_response: 0.15          # Very short response penalty
    repetition: 0.30              # Repetitive text penalty
    no_keyword_overlap: 0.20      # No common keywords with query
    low_keyword_overlap: 0.10     # Few common keywords

  # Detection thresholds
  thresholds:
    keyword_overlap_very_low: 0.05  # < 5% overlap
    keyword_overlap_low: 0.15       # < 15% overlap
    repetition_min_length: 20       # Min pattern length for repetition
    repetition_threshold: 3         # Pattern repeat count

  # Quality score bounds
  bounds:
    min_quality: 0.1              # Floor for quality scores
    max_quality: 0.95             # Ceiling for quality scores

# Implicit Feedback Detection
# Detects user dissatisfaction signals (retries, errors, latency tolerance)
feedback:
  retry_detection:
    similarity_threshold: 0.85    # >= 85% similar = likely retry
    time_window_seconds: 300.0    # 5-minute window for retry detection

  latency_detection:
    high_tolerance_max: 10.0      # Fast response threshold (< 10s)
    medium_tolerance_max: 30.0    # Acceptable speed threshold (10-30s)
    # Anything > 30s = slow (low tolerance, but user waited)

    # Reward mapping for implicit latency tolerance
    high_tolerance_reward: 0.9    # Fast response bonus
    medium_tolerance_reward: 0.7  # Acceptable speed
    low_tolerance_reward: 0.5     # Slow but user was patient

  error_detection:
    min_response_chars: 10        # Minimum chars before checking for errors
    error_patterns:               # List of error indicator strings
      - "I apologize, but I"
      - "I cannot"
      - "I'm sorry, but"
      - "Error:"
      - "Exception:"

  # Feedback weighting (explicit vs implicit)
  weights:
    explicit: 0.7                 # Weight on explicit user ratings
    implicit: 0.3                 # Weight on implicit behavioral signals

  # Implicit signal rewards
  rewards:
    error: 0.0                    # Error signal reward
    retry: 0.3                    # Retry signal reward

# Hybrid Routing Configuration
# Controls UCB1 (fast warm-start) → LinUCB (contextual) transition
#
# Switch threshold should be 5-10x feature dimensionality for LinUCB convergence:
#   64 PCA components (66 total dims): 500-1000 queries
#   128 PCA components (130 total dims): 1000-2000 queries
#   192 PCA components (194 total dims): 1500-3000 queries
hybrid_routing:
  switch_threshold: 2000          # Switch to LinUCB after N queries (adjust based on PCA dims)
  ucb1_c: 1.5                     # UCB1 exploration parameter
  linucb_alpha: 1.0               # LinUCB exploration parameter

# Arbiter LLM-as-Judge Configuration
# Controls quality evaluation using LLM-based assessment
arbiter:
  sample_rate: 0.1                # Evaluate 10% of responses (cost control)
  daily_budget: 10.0              # Maximum $10/day on evaluations
  model: "gpt-5"                  # Judge model for evaluation
  evaluators:                     # Active evaluator types
    - semantic
    - factuality

# Pricing Configuration
# Intelligent pricing management with database, cache, and live fetch fallbacks
pricing:
  # Cache freshness threshold (hours)
  # When using cache-only mode (no database), pricing is auto-refreshed if older than this
  # Default: 24 hours (conservative - pricing changes infrequently)
  cache_ttl_hours: 24

  # Database staleness warning threshold (days)
  # When using database mode, warn if latest pricing snapshot is older than this
  # Default: 7 days (pricing changes monthly/quarterly, but warn weekly)
  database_stale_days: 7

  # Fail-fast on stale cache errors
  # If true, fail immediately when cache is stale and llm-prices.com fetch fails
  # If false, use stale cache with warning (graceful degradation)
  # Default: true (prefer reliability over availability)
  fail_on_stale_cache: true

# Cache Configuration
# Redis caching for query features and routing decisions
cache:
  enabled: true                   # Enable Redis caching
  ttl: 86400                      # Feature cache TTL (24 hours in seconds)
  max_retries: 3                  # Maximum retry attempts for Redis ops
  timeout: 5                      # Redis operation timeout (seconds)

  circuit_breaker:
    threshold: 5                  # Failures before opening circuit
    timeout: 300                  # Circuit open duration (5 minutes)

  # Query history retention for retry detection
  history_ttl: 300                # 5-minute history retention

# LiteLLM Integration
# Model ID translation between LiteLLM format and Conduit standardized IDs
litellm:
  # Model ID mappings: LiteLLM format → Conduit format
  # Update when providers release new model versions
  model_mappings:
    # OpenAI
    gpt-5-nano-2025-08-07: gpt-5-nano
    gpt-5-mini-2025-08-07: gpt-5-mini
    gpt-5.1-2025-11-13: gpt-5.1

    # Anthropic
    claude-haiku-4-5-20251001: claude-haiku-4.5
    claude-sonnet-4-5-20250929: claude-sonnet-4.5
    claude-opus-4-5-20251101: claude-opus-4.5

    # Google
    gemini-2.5-flash: gemini-2.5-flash
    gemini-2.5-pro: gemini-2.5-pro

