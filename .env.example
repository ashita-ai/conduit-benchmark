# ============================================================================
# LLM Provider API Keys (for PydanticAI)
# ============================================================================

# OpenAI (GPT-4, GPT-4o, GPT-4o-mini)
# Get your key: https://platform.openai.com/api-keys
OPENAI_API_KEY=

# Anthropic (Claude 3.5 Sonnet, Claude Opus 4)
# Get your key: https://console.anthropic.com/
ANTHROPIC_API_KEY=

# Google (Gemini 1.5 Pro, Gemini 1.5 Flash)
# Get your key: https://makersuite.google.com/app/apikey
# NOTE: PydanticAI uses GEMINI_API_KEY (not GOOGLE_API_KEY)
GEMINI_API_KEY=

# Groq (fast inference - Llama, Mixtral)
# Get your key: https://console.groq.com/keys
GROQ_API_KEY=

# Mistral (Mistral Large, Mixtral)
# Get your key: https://console.mistral.ai/
MISTRAL_API_KEY=

# Cohere (Command R+, Embed v3)
# Get your key: https://dashboard.cohere.com/api-keys
COHERE_API_KEY=

# ============================================================================
# Database Configuration (Conduit + Benchmarking)
# ============================================================================

# Supabase
SUPABASE_URL=https://your-project.supabase.co
SUPABASE_ANON_KEY=your_supabase_anon_key_here

# PostgreSQL Connection
# Format: postgresql://user:password@host:port/database
DATABASE_URL=postgresql://postgres:password@localhost:5432/conduit_bench

# ============================================================================
# Redis (for Conduit caching)
# ============================================================================

# Local development
REDIS_URL=redis://localhost:6379

# Production (Redis Cloud, Upstash, etc.)
# REDIS_URL=redis://username:password@host:port

# ============================================================================
# Application Configuration
# ============================================================================

LOG_LEVEL=INFO
ENVIRONMENT=development

# ============================================================================
# Benchmarking Configuration
# ============================================================================

# Query generation
BENCHMARK_QUERY_COUNT=5000
BENCHMARK_CATEGORIES=technical_qa,creative_writing,analytical,conversational,summarization

# Evaluation
BENCHMARK_EVALUATION_MODEL=gpt-4o-mini
BENCHMARK_BATCH_SIZE=50

# Results storage
BENCHMARK_RESULTS_DIR=results/
